{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfIt5jLOoBcp",
        "outputId": "367918c7-649a-4490-dbb9-59f35557ffd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-04f767f61d8b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from konlpy.tag import Okt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Okt\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install --upgrade pyLDAvis\n",
        "!pip install --upgrade pandas\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis.gensim_models\n"
      ],
      "metadata": {
        "id": "VDkOufkCoNBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "불용어 제거"
      ],
      "metadata": {
        "id": "gatbeI8ZpGxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_file = 'stopword.txt'\n",
        "def load_stopwords(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        stopwords = {line.strip() for line in f}\n",
        "    return stopwords\n",
        "stop_words = load_stopwords(stopwords_file)"
      ],
      "metadata": {
        "id": "GeO19FNBoGgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA 모델링"
      ],
      "metadata": {
        "id": "SfDQ3q0cpDyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# DataFrame에서 요약 가져오기\n",
        "df = pd.read_csv('fail2.csv')\n",
        "summaries = df['요약'].tolist()\n",
        "\n",
        "# 텍스트 전처리를 위한 함수들 설정\n",
        "okt = Okt()\n",
        "stop_words = []\n",
        "\n",
        "# 텍스트 전처리 함수 정의\n",
        "def preprocess_text(sentence):\n",
        "    # 문장에서 형태소와 품사 정보 추출\n",
        "    tokens_with_pos = okt.pos(sentence)\n",
        "    # 명사와 형용사만 선택\n",
        "    tokens = [word for word, pos in tokens_with_pos if pos in ['Noun', 'Adjective']]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# 토픽 수 범위 설정\n",
        "min_topics = 2\n",
        "max_topics = 20\n",
        "step = 2\n",
        "\n",
        "# 최적의 토픽 수 탐색\n",
        "best_num_topics = None\n",
        "best_coherence_score = -1\n",
        "\n",
        "for num_topics in range(min_topics, max_topics + 1, step):\n",
        "    # 텍스트 전처리를 거친 요약들의 리스트\n",
        "    processed_summaries = [preprocess_text(summary) for summary in summaries]\n",
        "\n",
        "    # 사전과 코퍼스 생성\n",
        "    dictionary = corpora.Dictionary(processed_summaries)\n",
        "    corpus = [dictionary.doc2bow(text) for text in processed_summaries]\n",
        "\n",
        "    # LDA 모델 학습\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "    # 토픽 일관성 점수 계산\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=processed_summaries, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "    # 최적의 토픽 수 갱신\n",
        "    if coherence_score > best_coherence_score:\n",
        "        best_num_topics = num_topics\n",
        "        best_coherence_score = coherence_score\n",
        "\n",
        "# 최적의 토픽 수 출력\n",
        "print(f\"Best number of topics: {best_num_topics}\")\n",
        "\n",
        "# 최적의 토픽 수로 LDA 모델 학습\n",
        "processed_summaries = [preprocess_text(summary) for summary in summaries]\n",
        "dictionary = corpora.Dictionary(processed_summaries)\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_summaries]\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=best_num_topics)\n",
        "\n",
        "# 토픽별로 가장 중요한 단어들을 출력\n",
        "topics = lda_model.show_topics(num_topics=best_num_topics, num_words=5)\n",
        "for topic_id, topic_words in topics:\n",
        "    print(f\"Topic #{topic_id + 1}: {topic_words}\")\n",
        "\n",
        "# 각 요약에 대한 토픽 분포 출력\n",
        "for i, text in enumerate(processed_summaries):\n",
        "    bow = dictionary.doc2bow(text)\n",
        "    topic_distribution = lda_model.get_document_topics(bow)\n",
        "    for topic_id, prob in topic_distribution:\n",
        "        words = [word for word, prob in lda_model.show_topic\n"
      ],
      "metadata": {
        "id": "B2kHxBLBo4N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "젠심"
      ],
      "metadata": {
        "id": "07Nr1ZqApZW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(data)"
      ],
      "metadata": {
        "id": "tjmchdXwpVzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "워드클라우드"
      ],
      "metadata": {
        "id": "5R7TmhwMpbF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 텍스트 전처리를 거친 요약들의 리스트\n",
        "processed_summaries = [preprocess_text(summary) for summary in summaries]\n",
        "\n",
        "# 토큰을 하나의 리스트로 펼치기\n",
        "all_tokens = [token for sublist in processed_summaries for token in sublist]\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform([' '.join(text) for text in processed_summaries])\n",
        "\n",
        "# 토큰의 TF-IDF 가중치 계산\n",
        "token_weights = {}\n",
        "for idx, token in enumerate(tfidf.get_feature_names()):\n",
        "    token_weights[token] = tfidf_matrix[:, idx].mean()\n",
        "\n",
        "# 가장 높은 TF-IDF 가중치를 가진 상위 10개 토큰 추출\n",
        "top_tokens = sorted(token_weights.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "# 워드 클라우드 생성\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
        "wordcloud.generate_from_frequencies(dict(top_tokens))\n",
        "\n",
        "# 워드 클라우드 시각화\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gt8Xj9qdoOkH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}